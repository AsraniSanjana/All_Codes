{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWd3xbcuIFSpZYrX3vYwYa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AsraniSanjana/All_Codes/blob/main/All_Semester_Codes/ML_sem7/models/practice_id3_c4_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    \"\"\"Contains the information of the node and another nodes of the Decision Tree.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.value = None\n",
        "        self.next = None\n",
        "        self.childs = None\n",
        "\n",
        "class DecisionTreeClassifier:\n",
        "    \"\"\"Decision Tree Classifier using ID3 algorithm.\"\"\"\n",
        "\n",
        "    def __init__(self, X, feature_names, labels):\n",
        "        self.X = X  # features or predictors\n",
        "        self.feature_names = feature_names  # name of the features\n",
        "        self.labels = labels  # categories\n",
        "        self.labelCategories = list(set(labels))  # unique categories\n",
        "        # number of instances of each category\n",
        "        self.labelCategoriesCount = [list(labels).count(x) for x in self.labelCategories]\n",
        "        self.node = None  # nodes\n",
        "        # calculate the initial entropy of the system\n",
        "        self.entropy = self._get_entropy([x for x in range(len(self.labels))])\n",
        "\n",
        "\n",
        "    def _get_entropy(self, x_ids):\n",
        "        \"\"\" Calculates the entropy.\n",
        "        Parameters\n",
        "        __________\n",
        "        :param x_ids: list, List containing the instances ID's\n",
        "        __________\n",
        "        :return: entropy: float, Entropy.\n",
        "        \"\"\"\n",
        "        # sorted labels by instance id\n",
        "        labels = [self.labels[i] for i in x_ids]\n",
        "        # count number of instances of each category\n",
        "        label_count = [labels.count(x) for x in self.labelCategories]\n",
        "        # calculate the entropy for each category and sum them\n",
        "        entropy = sum([-count / len(x_ids) * math.log(count / len(x_ids), 2)\n",
        "                      if count else 0\n",
        "                      for count in label_count\n",
        "                      ])\n",
        "\n",
        "        return entropy\n",
        "\n",
        "    def _get_information_gain(self, x_ids, feature_id):\n",
        "        \"\"\"Calculates the information gain for a given feature based on its entropy and the total entropy of the system.\n",
        "        Parameters\n",
        "        __________\n",
        "        :param x_ids: list, List containing the instances ID's\n",
        "        :param feature_id: int, feature ID\n",
        "        __________\n",
        "        :return: info_gain: float, the information gain for a given feature.\n",
        "        \"\"\"\n",
        "        # calculate total entropy\n",
        "        info_gain = self._get_entropy(x_ids)\n",
        "        # store in a list all the values of the chosen feature\n",
        "        x_features = [self.X[x][feature_id] for x in x_ids]\n",
        "        # get unique values\n",
        "        feature_vals = list(set(x_features))\n",
        "        # get frequency of each value\n",
        "        feature_v_count = [x_features.count(x) for x in feature_vals]\n",
        "        # get the feature values ids\n",
        "        feature_v_id = [\n",
        "            [x_ids[i]\n",
        "            for i, x in enumerate(x_features)\n",
        "            if x == y]\n",
        "            for y in feature_vals\n",
        "        ]\n",
        "\n",
        "        # compute the information gain with the chosen feature\n",
        "        info_gain_feature = sum([v_counts / len(x_ids) * self._get_entropy(v_ids)\n",
        "                            for v_counts, v_ids in zip(feature_v_count, feature_v_id)])\n",
        "\n",
        "        info_gain = info_gain - info_gain_feature\n",
        "\n",
        "        return info_gain\n",
        "\n",
        "\n",
        "    def _get_feature_max_information_gain(self, x_ids, feature_ids):\n",
        "        \"\"\"Finds the attribute/feature that maximizes the information gain.\n",
        "        Parameters\n",
        "        __________\n",
        "        :param x_ids: list, List containing the samples ID's\n",
        "        :param feature_ids: list, List containing the feature ID's\n",
        "        __________\n",
        "        :returns: string and int, feature and feature id of the feature that maximizes the information gain\n",
        "        \"\"\"\n",
        "        # get the entropy for each feature\n",
        "        features_entropy = [self._get_information_gain(x_ids, feature_id) for feature_id in feature_ids]\n",
        "        # find the feature that maximises the information gain\n",
        "        max_id = feature_ids[features_entropy.index(max(features_entropy))]\n",
        "\n",
        "        return self.feature_names[max_id], max_id\n",
        "\n",
        "\n",
        "    def id3(self):\n",
        "        \"\"\"Initializes ID3 algorithm to build a Decision Tree Classifier.\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        # assign an unique number to each instance\n",
        "        x_ids = [x for x in range(len(self.X))]\n",
        "        # assign an unique number to each featuer\n",
        "        feature_ids = [x for x in range(len(self.feature_names))]\n",
        "        # define node variable - instance of the class Node\n",
        "        self.node = self._id3_recv(x_ids, feature_ids, self.node)\n",
        "\n",
        "\n",
        "    def _id3_recv(self, x_ids, feature_ids, node):\n",
        "        \"\"\"ID3 algorithm. It is called recursively until some criteria is met.\n",
        "        Parameters\n",
        "        __________\n",
        "        :param x_ids: list, list containing the samples ID's\n",
        "        :param feature_ids: list, List containing the feature ID's\n",
        "        :param node: object, An instance of the class Nodes\n",
        "        __________\n",
        "        :returns: An instance of the class Node containing all the information of the nodes in the Decision Tree\n",
        "        \"\"\"\n",
        "        if not node:\n",
        "            node = Node()  # initialize nodes\n",
        "        # sorted labels by instance id\n",
        "        labels_in_features = [self.labels[x] for x in x_ids]\n",
        "        # if all the example have the same class (pure node), return node\n",
        "        if len(set(labels_in_features)) == 1:\n",
        "            node.value = self.labels[x_ids[0]]\n",
        "            return node\n",
        "        # if there are not more feature to compute, return node with the most probable class\n",
        "        if len(feature_ids) == 0:\n",
        "            node.value = max(set(labels_in_features), key=labels_in_features.count)  # compute mode\n",
        "            return node\n",
        "        # else...\n",
        "        # choose the feature that maximizes the information gain\n",
        "        best_feature_name, best_feature_id = self._get_feature_max_information_gain(x_ids, feature_ids)\n",
        "        node.value = best_feature_name\n",
        "        node.childs = []\n",
        "        # value of the chosen feature for each instance\n",
        "        feature_values = list(set([self.X[x][best_feature_id] for x in x_ids]))\n",
        "        # loop through all the values\n",
        "        for value in feature_values:\n",
        "            child = Node()\n",
        "            child.value = value  # add a branch from the node to each feature value in our feature\n",
        "            node.childs.append(child)  # append new child node to current node\n",
        "            child_x_ids = [x for x in x_ids if self.X[x][best_feature_id] == value]\n",
        "            if not child_x_ids:\n",
        "                child.next = max(set(labels_in_features), key=labels_in_features.count)\n",
        "                print('')\n",
        "            else:\n",
        "                if feature_ids and best_feature_id in feature_ids:\n",
        "                    to_remove = feature_ids.index(best_feature_id)\n",
        "                    feature_ids.pop(to_remove)\n",
        "                # recursively call the algorithm\n",
        "                child.next = self._id3_recv(child_x_ids, feature_ids, child.next)\n",
        "\n",
        "    # if all the example have the same class (pure node), return node\n",
        "                if len(set(labels_in_features)) == 1:\n",
        "                    node.value = self.labels[x_ids[0]]\n",
        "                    return node\n",
        "\n",
        "                # if there are not more feature to compute, return node with the most probable class\n",
        "                if len(feature_ids) == 0:\n",
        "                    node.value = max(set(labels_in_features), key=labels_in_features.count)  # compute mode\n",
        "                    return node\n",
        "\n",
        "                  # else...\n",
        "        # choose the feature that maximizes the information gain\n",
        "                best_feature_name, best_feature_id = self._get_feature_max_information_gain(x_ids, feature_ids)\n",
        "                node.value = best_feature_name\n",
        "                node.childs = []\n",
        "                # value of the chosen feature for each instance\n",
        "                feature_values = list(set([self.X[x][best_feature_id] for x in x_ids]))\n",
        "                # loop through all the values\n",
        "                for value in feature_values:\n",
        "                    child = Node()\n",
        "                    child.value = value  # add a branch from the node to each feature value in our feature\n",
        "                    node.childs.append(child)  # append new child node to current node\n",
        "                    child_x_ids = [x for x in x_ids if self.X[x][best_feature_id] == value] # instances that take the branch\n",
        "                    if not child_x_ids:\n",
        "                        child.next = max(set(labels_in_features), key=labels_in_features.count)\n",
        "                        print('')\n",
        "                    else:\n",
        "                        if feature_ids and best_feature_id in feature_ids:\n",
        "                            to_remove = feature_ids.index(best_feature_id)\n",
        "                            feature_ids.pop(to_remove)\n",
        "                        # recursively call the algorithm\n",
        "                        child.next = self._id3_recv(child_x_ids, feature_ids, child.next)\n",
        "                return node\n",
        "        return node"
      ],
      "metadata": {
        "id": "7c0KDj0vEqsS"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "# Define the Node class and DecisionTreeClassifier class here\n",
        "\n",
        "# Load the dataset\n",
        "data_df = pd.read_csv(\"Buy_Computer.csv\")\n",
        "\n",
        "\n",
        "# Extract features, feature_names, and labels from the dataset\n",
        "X = data_df[['age', 'income', 'student', 'credit_rating']].values\n",
        "feature_names = ['age', 'income', 'student', 'credit_rating']\n",
        "labels = data_df['Buy_Computer'].values\n",
        "\n",
        "# Create an instance of DecisionTreeClassifier\n",
        "dt_classifier = DecisionTreeClassifier(X, feature_names, labels)\n",
        "\n",
        "# Run the ID3 algorithm\n",
        "dt_classifier.id3()\n"
      ],
      "metadata": {
        "id": "9nojWeL8IuIB"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "data_df = pd.read_csv(\"Buy_Computer.csv\") # Buy_computer here is the dataset name\n",
        "\n",
        "class_labels = data_df['Buy_Computer'] # Buy_Computer here is the last colmn name : class label : yes or no\n",
        "dt_classifier = DecisionTreeClassifier(None,None,class_labels)  # Pass None for X and feature_names\n"
      ],
      "metadata": {
        "id": "tklMbADXKpY2"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FOR ID3:**\n",
        "\n",
        "\n",
        "1.   ENTROPY\n",
        "2.   INFO GAIN\n",
        "3.   GAIN\n",
        "\n",
        "**FOR C4.5:**\n",
        "\n",
        "1.   GAIN RATIO\n",
        "2.   SPLIT INFO\n",
        "\n",
        "\n",
        "**FOR CART:**\n",
        "\n",
        "1.   GINI INDEX\n",
        "2.   DELTA GINI"
      ],
      "metadata": {
        "id": "Bgrh-PE9_6lP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getGain(attribute_name):\n",
        "    attribute_column = data_df[attribute_name]\n",
        "    class_labels = data_df['Buy_Computer']\n",
        "\n",
        "    # Create an instance of DecisionTreeClassifier\n",
        "    dt_classifier = DecisionTreeClassifier(None, None, class_labels)  # Pass None for X and feature_names\n",
        "\n",
        "    # Calculate entropy for the attribute values\n",
        "    attribute_values = attribute_column.unique()\n",
        "\n",
        "    entropies = []\n",
        "    weighted_entropies = []\n",
        "    values_counts = {}\n",
        "    for value in attribute_values:\n",
        "        subset_indices = attribute_column[attribute_column == value].index\n",
        "        entropy = dt_classifier._get_entropy(subset_indices)\n",
        "        entropies.append(entropy)\n",
        "        weight = len(subset_indices) / len(attribute_column)\n",
        "        weighted_entropy = entropy * weight\n",
        "        weighted_entropies.append(weighted_entropy)\n",
        "        values_counts[value] = len(subset_indices)\n",
        "        print(f\"Entropy for {attribute_name} = {value}: {entropy:.4f} , weight = {weight:.4f}\")\n",
        "\n",
        "\n",
        "    # Calculate information gain for the attribute\n",
        "    total_entropy = dt_classifier._get_entropy(attribute_column.index)\n",
        "    total_weighted_entropy = sum(weighted_entropies)\n",
        "    info_gain = total_weighted_entropy\n",
        "\n",
        "    # Calculate and display the final gain for the attribute\n",
        "    dataset_entropy = dt_classifier._get_entropy(attribute_column.index)\n",
        "    final_gain = dataset_entropy - info_gain\n",
        "\n",
        "\n",
        "    # Calculate split info for the attribute\n",
        "    total_tuples = len(data_df)\n",
        "    split_info = 0\n",
        "    for value_count in values_counts.values():\n",
        "        ratio = value_count / total_tuples\n",
        "        split_info -= ratio * math.log2(ratio)\n",
        "\n",
        "    gain_ratio = final_gain / split_info\n",
        "\n",
        "    # Print the Gain Ratio\n",
        "    print(f\"Gain Ratio({attribute_name}) = {final_gain:.4f} / {split_info:.4f} = {gain_ratio:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "    # Print the values counts and entropies in a table format\n",
        "    print(\"\\nValues Counts and Entropies:\")\n",
        "    print(f\"{attribute_name}\\tBuys_Comp = Yes\\tBuys_Comp = No\\tTotal\\t\\tEntropy\")\n",
        "    for value in values_counts:\n",
        "        yes_count = sum((data_df['Buy_Computer'][index] == 'yes') for index in data_df[data_df[attribute_name] == value].index)\n",
        "        no_count = values_counts[value] - yes_count\n",
        "        total_count = values_counts[value]\n",
        "        entropy = entropies[attribute_values.tolist().index(value)]\n",
        "        print(f\"{value}\\t\\t\\t{yes_count}\\t\\t\\t{no_count}\\t\\t{total_count}\\t\\t{entropy:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"\\nHence, Gain({attribute_name}) becomes {dataset_entropy:.4f} - {info_gain:.4f} = {final_gain:.4f}\")\n",
        "    print(f\"Split Info({attribute_name}) = {split_info:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "# Call the function for 'income' attribute\n",
        "getGain('age')\n",
        "getGain('income')\n",
        "getGain('student')\n",
        "getGain('credit_rating')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nlbgxfjYXIh",
        "outputId": "87007387-2b5e-462c-f272-94c0dd3b457c"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy for age = youth: 0.9710 , weight = 0.3571\n",
            "Entropy for age = middle_age: 0.0000 , weight = 0.2857\n",
            "Entropy for age = senior: 0.9710 , weight = 0.3571\n",
            "Gain Ratio(age) = 0.2467 / 1.5774 = 0.1564\n",
            "\n",
            "Values Counts and Entropies:\n",
            "age\tBuys_Comp = Yes\tBuys_Comp = No\tTotal\t\tEntropy\n",
            "youth\t\t\t2\t\t\t3\t\t5\t\t0.9710\n",
            "middle_age\t\t\t4\t\t\t0\t\t4\t\t0.0000\n",
            "senior\t\t\t3\t\t\t2\t\t5\t\t0.9710\n",
            "\n",
            "Hence, Gain(age) becomes 0.9403 - 0.6935 = 0.2467\n",
            "Split Info(age) = 1.5774\n",
            "Entropy for income = high: 1.0000 , weight = 0.2857\n",
            "Entropy for income = medium: 0.9183 , weight = 0.4286\n",
            "Entropy for income = low: 0.8113 , weight = 0.2857\n",
            "Gain Ratio(income) = 0.0292 / 1.5567 = 0.0188\n",
            "\n",
            "Values Counts and Entropies:\n",
            "income\tBuys_Comp = Yes\tBuys_Comp = No\tTotal\t\tEntropy\n",
            "high\t\t\t2\t\t\t2\t\t4\t\t1.0000\n",
            "medium\t\t\t4\t\t\t2\t\t6\t\t0.9183\n",
            "low\t\t\t3\t\t\t1\t\t4\t\t0.8113\n",
            "\n",
            "Hence, Gain(income) becomes 0.9403 - 0.9111 = 0.0292\n",
            "Split Info(income) = 1.5567\n",
            "Entropy for student = no: 0.9852 , weight = 0.5000\n",
            "Entropy for student = yes: 0.5917 , weight = 0.5000\n",
            "Gain Ratio(student) = 0.1518 / 1.0000 = 0.1518\n",
            "\n",
            "Values Counts and Entropies:\n",
            "student\tBuys_Comp = Yes\tBuys_Comp = No\tTotal\t\tEntropy\n",
            "no\t\t\t3\t\t\t4\t\t7\t\t0.9852\n",
            "yes\t\t\t6\t\t\t1\t\t7\t\t0.5917\n",
            "\n",
            "Hence, Gain(student) becomes 0.9403 - 0.7885 = 0.1518\n",
            "Split Info(student) = 1.0000\n",
            "Entropy for credit_rating = fair: 0.8113 , weight = 0.5714\n",
            "Entropy for credit_rating = excellent: 1.0000 , weight = 0.4286\n",
            "Gain Ratio(credit_rating) = 0.0481 / 0.9852 = 0.0488\n",
            "\n",
            "Values Counts and Entropies:\n",
            "credit_rating\tBuys_Comp = Yes\tBuys_Comp = No\tTotal\t\tEntropy\n",
            "fair\t\t\t6\t\t\t2\t\t8\t\t0.8113\n",
            "excellent\t\t\t3\t\t\t3\t\t6\t\t1.0000\n",
            "\n",
            "Hence, Gain(credit_rating) becomes 0.9403 - 0.8922 = 0.0481\n",
            "Split Info(credit_rating) = 0.9852\n"
          ]
        }
      ]
    }
  ]
}