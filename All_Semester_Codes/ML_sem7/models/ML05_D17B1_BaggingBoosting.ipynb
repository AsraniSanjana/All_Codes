{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsaF54AqCnBXT3M+3FQ7/W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AsraniSanjana/All_Codes/blob/main/All_Semester_Codes/ML_sem7/models/ML05_D17B1_BaggingBoosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NAME: SANJANA ASRANI\n",
        "\n",
        "DIV: D17B\n",
        "\n",
        "ROLL NO.: 01\n",
        "\n",
        "ML LAB 05 : BAGGING AND BOOSTING\n",
        "\n",
        "DOP : 25/09/23"
      ],
      "metadata": {
        "id": "qAElo_EJp2n9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BAGGING VS. BOOSTING\n",
        "\n",
        "**Bagging (Bootstrap Aggregating)**:\n",
        "\n",
        "1. **Base Model Training**:\n",
        "   - Trains multiple base models independently in parallel.\n",
        "   - Each base model is trained on a random subset of the training data (with replacement).\n",
        "   - No dependency among the base models.\n",
        "\n",
        "2. **Weighting of Data**:\n",
        "   - All base models contribute equally to the final prediction.\n",
        "   - No specific weighting of base models.\n",
        "   - Final prediction is often an average (in Bagging Regressor) or majority vote (in Bagging Classifier) of the base model predictions.\n",
        "\n",
        "3. **Variance vs. Bias**:\n",
        "   - Aims to reduce variance.\n",
        "   - Reduces the variability of predictions and improves model stability.\n",
        "\n",
        "4. **Parallelism**:\n",
        "   - Suitable for parallel and distributed computing because base models are trained independently.\n",
        "\n",
        "5. **Robustness to Overfitting**:\n",
        "   - Tends to reduce overfitting, making it less prone to overfit the training data.\n",
        "   - More likely to generalize well to unseen data.\n",
        "\n",
        "**Boosting**:\n",
        "\n",
        "1. **Base Model Training**:\n",
        "   - Trains multiple base models sequentially in an adaptive manner.\n",
        "   - Each base model corrects the errors made by the previous models.\n",
        "   - Base models are usually weak learners (models that perform slightly better than random chance).\n",
        "\n",
        "2. **Weighting of Data**:\n",
        "   - Base models are assigned weights based on their performance.\n",
        "   - Models that perform better on the training data are given higher weights.\n",
        "   - Models that perform worse are given lower weights.\n",
        "   - Each base model's contribution to the final prediction is weighted according to its competence.\n",
        "\n",
        "3. **Variance vs. Bias**:\n",
        "   - Aims to reduce bias.\n",
        "   - Focuses on correcting errors made by previous models.\n",
        "   - Can result in models with low bias but potentially higher variance.\n",
        "\n",
        "4. **Parallelism**:\n",
        "   - Limited parallelism because base models are trained sequentially.\n",
        "   - Each base model depends on the output of the previous model, which limits parallelization.\n",
        "\n",
        "5. **Robustness to Overfitting**:\n",
        "   - Can be more prone to overfitting, especially with too many iterations or complex base models.\n",
        "   - Requires careful tuning to prevent overfitting."
      ],
      "metadata": {
        "id": "7O7A2-1MqBU9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYgHq5p94qYI"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target)  # Target variable (species)"
      ],
      "metadata": {
        "id": "PALmUdOxwcsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mapping target values to numbers\n",
        "class_mapping = {\n",
        "    0: 0,\n",
        "    1: 1,\n",
        "    2: 2\n",
        "}\n",
        "y = y.map(class_mapping)"
      ],
      "metadata": {
        "id": "hwTNEO1YwfM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "n_estimators = 5000  # Number of base estimators (e.g., Decision Trees)\n",
        "bagging_regressor = BaggingRegressor(n_estimators=n_estimators, random_state=42)\n",
        "bagging_regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "B_zbs7BCwi7u",
        "outputId": "48434d69-f1f0-45eb-d336-24159eaa8e25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaggingRegressor(n_estimators=5000, random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BaggingRegressor(n_estimators=5000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BaggingRegressor</label><div class=\"sk-toggleable__content\"><pre>BaggingRegressor(n_estimators=5000, random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = bagging_regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiJFh6aTwlfy",
        "outputId": "cbaaab63-0d7e-467e-a3be-4899cab6ab1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.0009295680000000002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HYPER PARAMETER TUNING"
      ],
      "metadata": {
        "id": "aB-ct-VkqwBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bagging_regressor = BaggingRegressor(random_state=42)\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "\n",
        "# FOR 4 ESTIMATORS\n",
        "param_grid = {\n",
        "    'n_estimators': [25, 100, 150,250],        # Number of base estimators (Decision Trees)\n",
        "    'max_samples': [0.4, 0.4, 0.4, 0.4],       # Fraction of samples to be used for each base estimator\n",
        "    'max_features': [0.5, 0.8, 1.0,1.0],      # Fraction of features to be used for each base estimator\n",
        "}"
      ],
      "metadata": {
        "id": "wo81yQAosEq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=bagging_regressor, param_grid=param_grid,scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Perform grid search\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "id": "Xo5b2aeswrzL",
        "outputId": "d676a000-b97a-476d-eebc-5d46438ccbc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=BaggingRegressor(random_state=42),\n",
              "             param_grid={'max_features': [0.5, 0.8, 1.0, 1.0],\n",
              "                         'max_samples': [0.4, 0.4, 0.4, 0.4],\n",
              "                         'n_estimators': [25, 100, 150, 250]},\n",
              "             scoring='neg_mean_squared_error')"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=BaggingRegressor(random_state=42),\n",
              "             param_grid={&#x27;max_features&#x27;: [0.5, 0.8, 1.0, 1.0],\n",
              "                         &#x27;max_samples&#x27;: [0.4, 0.4, 0.4, 0.4],\n",
              "                         &#x27;n_estimators&#x27;: [25, 100, 150, 250]},\n",
              "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=BaggingRegressor(random_state=42),\n",
              "             param_grid={&#x27;max_features&#x27;: [0.5, 0.8, 1.0, 1.0],\n",
              "                         &#x27;max_samples&#x27;: [0.4, 0.4, 0.4, 0.4],\n",
              "                         &#x27;n_estimators&#x27;: [25, 100, 150, 250]},\n",
              "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: BaggingRegressor</label><div class=\"sk-toggleable__content\"><pre>BaggingRegressor(random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BaggingRegressor</label><div class=\"sk-toggleable__content\"><pre>BaggingRegressor(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRSyxPkNwuAC",
        "outputId": "4fcb6e80-ade2-4b13-8da7-d7a4bd5fb1a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "{'max_features': 0.8, 'max_samples': 0.4, 'n_estimators': 250}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model with the best hyperparameters\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (Best Model): {mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOmLKIQQwwhA",
        "outputId": "9dafffaf-2ff5-4e81-f7d2-2de7bdb77693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Best Model): 0.0046261333333333324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ASSESSMENT**"
      ],
      "metadata": {
        "id": "Oenc0Plg4vFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**What is the difference between BAGGING REGRESSOR AND BAGGING CLASSIFIER**\n",
        "\n",
        "(i) Bagging, which stands for Bootstrap Aggregating, is an ensemble learning technique that is used to improve the performance of machine learning models by reducing overfitting and variance and thus increasing stability.\n",
        "\n",
        "(ii) improves the overall performance of machine learning models by introducing diversity among the base models through bootstrapping.\n",
        "\n",
        "(iii) bagging can be applied to both regression and classification problems, resulting in Bagging Regressors and Bagging Classifiers, respectively. The main difference between them lies in the type of prediction task they are designed for:\n",
        "\n",
        "1. Bagging Regressor:\n",
        "   - bagging Regressor is used for regression tasks\n",
        "   - aggregates the predictions of multiple regression models to make a final prediction\n",
        "   - the goal is to predict a continuous numeric value (e.g., predicting house prices, temperature, stock prices)\n",
        "   - It works by training multiple base regression models on different subsets of the training data, which are created through bootstrapping (SRSWR - randomly sampling with replacement)\n",
        "   - Common base models used : decision trees or linear regression\n",
        "\n",
        "2. Bagging Classifier:\n",
        "   - Bagging Classifier is used for classification tasks\n",
        "   - The final prediction is determined by aggregating the predictions of the base classifiers through methods such as majority voting (for binary classification) or weighted voting (for multi-class classification).\n",
        "   - goal is to classify input data into one of several predefined classes or categories (e.g., spam detection, image recognition).\n",
        "   - Similar to the Bagging Regressor, it also involves training multiple base classifiers on bootstrapped subsets of the training data.\n",
        "   - Common base classifiers used in Bagging Classifiers include decision trees, random forests, and support vector machines.\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "dJrGO8_45IVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. What is the difference between BOOSTING REGRESSOR AND BOOSTING CLASSIFIER**\n",
        "\n",
        "Boosting is another ensemble learning technique used to improve the performance of machine learning models. Like bagging, boosting can be applied to both regression and classification problems, resulting in Boosting Regressors and Boosting Classifiers. The key difference between them is in how they train and combine base models:\n",
        "\n",
        "1. Boosting Regressor:\n",
        "   - Boosting Regressor is used for regression tasks, where the objective is to predict a continuous numeric value (e.g., predicting house prices, stock prices).\n",
        "   - It works by training a sequence of base regression models, with each subsequent model focusing on the mistakes made by the previous ones.\n",
        "   - Base models are typically decision trees with limited depth (weak learners).\n",
        "   - During training, each base model is assigned a weight based on its performance, and the models' predictions are combined through a weighted sum to make the final prediction.\n",
        "   - The process continues until a predefined number of base models (iterations) are trained or until the performance stops improving.\n",
        "\n",
        "2. Boosting Classifier:\n",
        "   - Boosting Classifier is used for classification tasks, where the goal is to classify input data into one of several predefined classes or categories (e.g., spam detection, image recognition).\n",
        "   - Similar to Boosting Regressor, it trains a sequence of base classifiers, with each one focusing on correcting the mistakes of its predecessors.\n",
        "   - Base classifiers are typically decision trees with limited depth (weak learners).\n",
        "   - The final prediction in Boosting Classifier is determined by a weighted combination of the base classifiers' predictions, with more weight given to the models that perform better on the training data.\n",
        "   - The boosting process continues for a set number of iterations or until no further improvements in performance can be achieved.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5mBuYnOc5ZTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. What are the different ways to combine classifiers.**\n",
        "\n",
        "The choice of which method to use depends on the nature of the problem, the characteristics of the data, and the behavior of the individual base classifiers. Ensemble methods are powerful tools for improving predictive performance and robustness in various machine learning tasks\n",
        "\n",
        "1. **Voting Methods**:\n",
        "   - **Majority Voting**: In binary classification, each classifier's prediction is treated as a vote, and the class with the majority of votes is chosen as the final prediction. In multi-class classification, the class with the highest number of votes can be selected.\n",
        "   - **Weighted Voting**: Similar to majority voting, but each classifier's vote is weighted based on its reliability or performance on the validation data.\n",
        "\n",
        "2. **Averaging Methods**:\n",
        "   - **Simple Average**: For regression tasks, the predictions of multiple regressors are averaged to obtain the final prediction.\n",
        "   - **Weighted Average**: Similar to simple averaging, but each regressor's prediction is weighted based on its performance or reliability.\n",
        "\n",
        "3. **Bagging (Bootstrap Aggregating)**:\n",
        "   - Bagging combines multiple base classifiers by training them independently on bootstrapped subsets of the training data. For classification tasks, the final prediction can be obtained by majority voting.\n",
        "\n",
        "4. **Boosting**:\n",
        "   - Boosting combines multiple base classifiers sequentially, with each classifier trained to correct the errors made by the previous ones. The final prediction is typically a weighted combination of the base classifier predictions.\n",
        "\n",
        "5. **Random Forests**:\n",
        "   - Random Forests combine multiple decision trees. Each tree is trained on a subset of the data and a random subset of features. The final prediction is obtained through majority voting (for classification) or averaging (for regression).\n",
        "\n",
        "8. **Adaptive Methods**:\n",
        "   - Methods like AdaBoost adaptively assign weights to data points during training to emphasize the samples that were misclassified by previous classifiers.\n",
        "   \n",
        "\n",
        " **Stacking (Stacked Generalization), Stacked Ensembles, Bayesian Model Averaging , Clustering Ensembles** are a few other methods.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "R36Yex515TSY"
      }
    }
  ]
}